# =============================================================================
# STEP 1: IMPORTARE LE LIBRERIE NECESSARIE
# =============================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("‚úÖ Librerie importate con successo!")

# =============================================================================
# STEP 2: CARICARE IL DATASET DA CSV
# =============================================================================

# Leggiamo il file CSV e lo carichiamo in un DataFrame pandas
df = pd.read_csv('./dataset/dati_LogisticRegression.csv')

# Visualizziamo le prime 5 righe per capire la struttura dei dati
print("üìä ANTEPRIMA DEL DATASET:")
print(df.head())

# Controlliamo le informazioni generali sul dataset
print("\nüìã INFORMAZIONI DATASET:")
print(df.info())

# Verifichiamo le statistiche descrittive
print("\nüìà STATISTICHE DESCRITTIVE:")
print(df.describe())

# =============================================================================
# STEP 3: PULIZIA E PREPARAZIONE DEI DATI
# =============================================================================

# Controlliamo se ci sono valori mancanti
print("\nüîç VALORI MANCANTI PER COLONNA:")
print(df.isnull().sum())

# Se ci fossero valori mancanti, potremmo gestirli cos√¨:
# df = df.dropna()  # Elimina righe con valori mancanti
# Oppure:
# df = df.fillna(df.mean())  # Riempie con la media

# SPIEGAZIONE COLONNE:
# age: Et√† del cliente
# balance: Saldo del conto
# estimated_salary: Stipendio stimato
# credit_score: Punteggio creditizio
# tenure: Anni di relazione con la banca
# number_of_products: Numero di prodotti bancari
# has_credit_card: Possiede carta di credito (1=Si, 0=No)
# is_active_member: Membro attivo (1=Si, 0=No)
# churn: Il cliente ha abbandonato? (1=Si, 0=No) ‚Üê TARGET

# Verifichiamo il bilanciamento delle classi target
print("\n‚öñÔ∏è DISTRIBUZIONE DELLA VARIABILE TARGET (churn):")
print(df['churn'].value_counts())# Valori assoluti
print("Proporzioni:")
print(df['churn'].value_counts(normalize=True)) # Valori percentuali

# =============================================================================
# STEP 5: PREPARAZIONE DEI DATI PER IL MODELLO
# =============================================================================

# Separiamo le features (variabili indipendenti) dal target (variabile dipendente)
# X = tutte le colonne tranne 'churn'
# y = solo la colonna 'churn'

X = df.drop('churn', axis=1)  # axis=1 significa che stiamo eliminando una colonna
y = df['churn']

print("\nüéØ FEATURES (X):")
print(X.head())
print(f"Dimensione di X: {X.shape}")  # (numero_righe, numero_colonne)

print("\nüéØ TARGET (y):")
print(y.head())
print(f"Dimensione di y: {y.shape}")

# Dividiamo il dataset in training set (80%) e test set (20%)
# random_state=42 garantisce risultati riproducibili
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,       # 20% dei dati per il test
    random_state=42,     # Seed per riproducibilit√†
    stratify=y           # Mantiene la stessa proporzione di classi in train e test
)

print(f"\nüìä DIVISIONE DATASET:")
print(f"Training set: {X_train.shape[0]} righe")
print(f"Test set: {X_test.shape[0]} righe")

# =============================================================================
# STEP 6: NORMALIZZAZIONE DELLE FEATURES
# =============================================================================

# La regressione logistica beneficia della normalizzazione dei dati
# StandardScaler sottrae la media e divide per la deviazione standard
# Questo mette tutte le feature sulla stessa scala

scaler = StandardScaler()

# Addestriamo lo scaler SOLO sul training set e trasformiamo entrambi i set
X_train_scaled = scaler.fit_transform(X_train)  # fit_transform: impara parametri e applica
X_test_scaled = scaler.transform(X_test)        # transform: applica stessa trasformazione

# print("\nüìè DATI NORMALIZZATI:")
# print("Prime 5 righe del training set normalizzato:")
# print(X_train_scaled[:5])

# # =============================================================================
# # STEP 7: CREAZIONE E ADDESTRAMENTO DEL MODELLO
# # =============================================================================

# # Creiamo l'istanza del modello di Regressione Logistica
# model = LogisticRegression(
#     random_state=42,     # Seed per riproducibilit√†
#     max_iter=1000,       # Numero massimo di iterazioni per convergenza
#     C=1.0,               # Parametro di regolarizzazione (pi√π piccolo = pi√π regolarizzazione)
#     solver='lbfgs',      # Algoritmo di ottimizzazione (buono per dataset piccoli/medi)
#     penalty='l2'         # Tipo di regolarizzazione (L2 = Ridge)
# )

# # Addestriamo il modello sui dati di training
# print("\nüöÄ ADDESTRAMENTO DEL MODELLO IN CORSO...")
# model.fit(X_train_scaled, y_train)
# print("‚úÖ Modello addestrato con successo!")

# # =============================================================================
# # STEP 8: SPIEGAZIONE DETTAGLIATA DEI PARAMETRI
# # =============================================================================

# print("\nüéì SPIEGAZIONE PARAMETRI DEL MODELLO:")

# # Coefficienti delle feature (importanza di ogni feature)
# coefficients = model.coef_[0]
# feature_importance = pd.DataFrame({
#     'Feature': X.columns,
#     'Coefficient': coefficients,
#     'Abs_Coefficient': np.abs(coefficients)
# }).sort_values('Abs_Coefficient', ascending=False)

# print("\nüìä IMPORTANZA DELLE FEATURE (Coefficienti):")
# print(feature_importance)

# # Interpretazione dei coefficienti:
# print("\nüí° COME INTERPRETARE I COEFFICIENTI:")
# print("‚Ä¢ Coefficiente POSITIVO: Aumenta la probabilit√† di churn (abbandono)")
# print("‚Ä¢ Coefficiente NEGATIVO: Diminuisce la probabilit√† di churn")
# print("‚Ä¢ Valore ASSOLUTO pi√π alto: Feature pi√π influente")

# # Intercetta (bias)
# print(f"\nüìê INTERCETTA (bias): {model.intercept_[0]:.4f}")
# print("L'intercetta rappresenta la probabilit√† logistica di base quando tutte le feature sono 0")

# # =============================================================================
# # STEP 9: VALUTAZIONE DEL MODELLO
# # =============================================================================

# # Facciamo previsioni sul test set
# y_pred = model.predict(X_test_scaled)
# y_pred_proba = model.predict_proba(X_test_scaled)  # Probabilit√† per ogni classe

# print("\nüìà VALUTAZIONE DEL MODELLO:")

# # 1. Accuratezza
# accuracy = accuracy_score(y_test, y_pred)
# print(f"üéØ ACCURATEZZA: {accuracy:.4f} ({accuracy*100:.2f}%)")

# # 2. Matrice di confusione
# print("\nüìä MATRICE DI CONFUSIONE:")
# cm = confusion_matrix(y_test, y_pred)
# print(cm)

# # Visualizziamo la matrice di confusione
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
# plt.title('Matrice di Confusione')
# plt.xlabel('Predetto')
# plt.ylabel('Reale')
# plt.show()

# # 3. Report di classificazione dettagliato
# print("\nüìã REPORT DI CLASSIFICAZIONE:")
# print(classification_report(y_test, y_pred))

# # =============================================================================
# # STEP 10: INTERPRETAZIONE DELLE PROBABILIT√Ä
# # =============================================================================

# # La regressione logistica fornisce probabilit√†, non solo classificazioni
# print("\nüé≤ PROBABILIT√Ä DI PREDIZIONE (prime 5 istanze del test set):")
# probabilities_df = pd.DataFrame({
#     'Probabilit√† Classe 0 (No Churn)': y_pred_proba[:, 0],
#     'Probabilit√† Classe 1 (Churn)': y_pred_proba[:, 1],
#     'Predizione Finale': y_pred,
#     'Valore Reale': y_test.values
# })
# print(probabilities_df.head())

# # =============================================================================
# # STEP 11: UTILIZZO CONCRETO DEL MODELLO
# # =============================================================================

# print("\nüöÄ UTILIZZO PRATICO DEL MODELLO:")

# # Esempio: Prevedere se un NUOVO cliente abbandoner√†
# nuovo_cliente = pd.DataFrame({
#     'age': [45],
#     'balance': [60000],
#     'estimated_salary': [90000],
#     'credit_score': [620],
#     'tenure': [4],
#     'number_of_products': [2],
#     'has_credit_card': [1],
#     'is_active_member': [1]
# })

# # Preprocessing del nuovo cliente
# nuovo_cliente_scaled = scaler.transform(nuovo_cliente)

# # Previsione
# probabilita_churn = model.predict_proba(nuovo_cliente_scaled)[0, 1]
# predizione = model.predict(nuovo_cliente_scaled)[0]

# print(f"\nüîÆ PREVISIONE PER NUOVO CLIENTE:")
# print(f"Probabilit√† di abbandono: {probabilita_churn:.4f} ({probabilita_churn*100:.2f}%)")
# print(f"Predizione: {'ABBADONER√Ä' if predizione == 1 else 'RIMARR√Ä'}")

# # Soglia di decisione (default 0.5, ma possiamo modificarla)
# soglia_personalizzata = 0.3  # Pi√π sensibile ai casi di churn
# predizione_soglia = (probabilita_churn > soglia_personalizzata).astype(int)

# print(f"\nüéöÔ∏è CON SOGLIA PERSONALIZZATA ({soglia_personalizzata}):")
# print(f"Predizione: {'ABBADONER√Ä' if predizione_soglia == 1 else 'RIMARR√Ä'}")

# # =============================================================================
# # STEP 12: QUANDO USARE E NON USARE LA REGRESSIONE LOGISTICA
# # =============================================================================

# print("\n" + "="*60)
# print("üéØ QUANDO USARE LA REGRESSIONE LOGISTICA:")
# print("="*60)
# print("‚úÖ PROBLEMI DI CLASSIFICAZIONE BINARIA")
# print("‚úÖ INTERPRETABILIT√Ä DEL MODELLO IMPORTANTE")
# print("‚úÖ RELAZIONI APPROSSIMATIVAMENTE LINEARI")
# print("‚úÖ DATASET DI DIMENSIONI PICCOLE/MEDIE")
# print("‚úÖ BASELINE MODEL per confrontare modelli pi√π complessi")

# print("\n" + "="*60)
# print("üö´ QUANDO NON USARE LA REGRESSIONE LOGISTICA:")
# print("="*60)
# print("‚ùå PROBLEMI DI CLASSIFICAZIONE MULTI-CLASSE (pi√π di 2 categorie)")
# print("‚ùå RELAZIONI COMPLESSE NON LINEARI")
# print("‚ùå DATASET MOLTO GRANDI E COMPLESSI")
# print("‚ùå FEATURE CON INTERAZIONI COMPLESSE")
# print("‚ùå PERFORMANCE MASSIME RICHIESTE (usare XGBoost, Neural Networks)")

# # =============================================================================
# # STEP 13: VISUALIZZAZIONE DELLE DECISIONI DEL MODELLO
# # =============================================================================

# # Visualizziamo come il modello prende decisioni per 2 feature principali
# if X.shape[1] >= 2:
#     plt.figure(figsize=(10, 8))
    
#     # Prendiamo le due feature pi√π importanti
#     top_features = feature_importance['Feature'].head(2).values
    
#     # Creiamo un grid per la visualizzazione
#     feature1 = X[top_features[0]]
#     feature2 = X[top_features[1]]
    
#     plt.scatter(feature1[y == 0], feature2[y == 0], alpha=0.7, label='No Churn', color='green')
#     plt.scatter(feature1[y == 1], feature2[y == 1], alpha=0.7, label='Churn', color='red')
    
#     plt.xlabel(top_features[0])
#     plt.ylabel(top_features[1])
#     plt.title('Distribuzione delle Classi per le Feature pi√π Importanti')
#     plt.legend()
#     plt.show()

# print("\nüéâ TUTORIAL COMPLETATO! La regressione logistica non ha pi√π segreti!")