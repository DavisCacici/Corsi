# =============================================================================
# STEP 1: IMPORTARE LE LIBRERIE NECESSARIE
# =============================================================================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("‚úÖ Librerie importate con successo!")

# =============================================================================
# STEP 2: CARICARE IL DATASET DA CSV
# =============================================================================

# Leggiamo il file CSV e lo carichiamo in un DataFrame pandas
df = pd.read_csv('./dataset/dati_LogisticRegression.csv')

# Visualizziamo le prime 5 righe per capire la struttura dei dati
print("üìä ANTEPRIMA DEL DATASET:")
print(df.head())

# Controlliamo le informazioni generali sul dataset
print("\nüìã INFORMAZIONI DATASET:")
print(df.info())

# Verifichiamo le statistiche descrittive
print("\nüìà STATISTICHE DESCRITTIVE:")
print(df.describe())

# =============================================================================
# STEP 3: PULIZIA E PREPARAZIONE DEI DATI
# =============================================================================

# Controlliamo se ci sono valori mancanti
print("\nüîç VALORI MANCANTI PER COLONNA:")
print(df.isnull().sum())

# Se ci fossero valori mancanti, potremmo gestirli cos√¨:
# df = df.dropna()  # Elimina righe con valori mancanti
# Oppure:
# df = df.fillna(df.mean())  # Riempie con la media

# SPIEGAZIONE COLONNE:
# age: Et√† del cliente
# balance: Saldo del conto
# estimated_salary: Stipendio stimato
# credit_score: Punteggio creditizio
# tenure: Anni di relazione con la banca
# number_of_products: Numero di prodotti bancari
# has_credit_card: Possiede carta di credito (1=Si, 0=No)
# is_active_member: Membro attivo (1=Si, 0=No)
# churn: Il cliente ha abbandonato? (1=Si, 0=No) ‚Üê TARGET

# Verifichiamo il bilanciamento delle classi target
print("\n‚öñÔ∏è DISTRIBUZIONE DELLA VARIABILE TARGET (churn):")
print(df['churn'].value_counts())# Valori assoluti
print("Proporzioni:")
print(df['churn'].value_counts(normalize=True)) # Valori percentuali

# =============================================================================
# STEP 4: PREPARAZIONE DEI DATI PER IL MODELLO
# =============================================================================

# Separiamo le features (variabili indipendenti) dal target (variabile dipendente)
# X = tutte le colonne tranne 'churn'
# y = solo la colonna 'churn'

X = df.drop('churn', axis=1)  # axis=1 significa che stiamo eliminando una colonna
y = df['churn']

print("\nüéØ FEATURES (X):")
print(X.head())
print(f"Dimensione di X: {X.shape}")  # (numero_righe, numero_colonne)

print("\nüéØ TARGET (y):")
print(y.head())
print(f"Dimensione di y: {y.shape}")

# Dividiamo il dataset in training set (80%) e test set (20%)
# random_state=42 garantisce risultati riproducibili
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,       # 20% dei dati per il test
    random_state=42,     # Seed per riproducibilit√†
    stratify=y           # Mantiene la stessa proporzione di classi in train e test
)

print(f"\nüìä DIVISIONE DATASET:")
print(f"Training set: {X_train.shape[0]} righe")
print(f"Test set: {X_test.shape[0]} righe")

# =============================================================================
# STEP 5: NORMALIZZAZIONE DELLE FEATURES
# =============================================================================

# La regressione logistica beneficia della normalizzazione dei dati
# StandardScaler sottrae la media e divide per la deviazione standard
# Questo mette tutte le feature sulla stessa scala

scaler = StandardScaler()

# Addestriamo lo scaler SOLO sul training set e trasformiamo entrambi i set
X_train_scaled = scaler.fit_transform(X_train)  # fit_transform: impara parametri e applica
X_test_scaled = scaler.transform(X_test)        # transform: applica stessa trasformazione

print("\nüìè DATI NORMALIZZATI:")
print("Prime 5 righe del training set normalizzato:")
print(X_train_scaled[:5])

# =============================================================================
# STEP 6: CREAZIONE E ADDESTRAMENTO DEL MODELLO
# =============================================================================

# Creiamo l'istanza del modello di Regressione Logistica
model = LogisticRegression(
    random_state=42,     # Seed per riproducibilit√†
    max_iter=1000,       # Numero massimo di iterazioni per convergenza
    C=1.0,               # Parametro di regolarizzazione (pi√π piccolo = pi√π regolarizzazione)
    solver='lbfgs',      # Algoritmo di ottimizzazione (buono per dataset piccoli/medi)
    penalty='l2'         # Tipo di regolarizzazione (L2 = Ridge)
)

# Addestriamo il modello sui dati di training
print("\nüöÄ ADDESTRAMENTO DEL MODELLO IN CORSO...")
model.fit(X_train_scaled, y_train)
print("‚úÖ Modello addestrato con successo!")

# =============================================================================
# STEP 7: SPIEGAZIONE DETTAGLIATA DEI PARAMETRI
# =============================================================================

print("\nüéì SPIEGAZIONE PARAMETRI DEL MODELLO:")

# Coefficienti delle feature (importanza di ogni feature)
coefficients = model.coef_[0]
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': coefficients,
    # 'Abs_Coefficient': np.abs(coefficients)
}).sort_values('Abs_Coefficient', ascending=False)

print("\nüìä IMPORTANZA DELLE FEATURE (Coefficienti):")
print(feature_importance)

# Interpretazione dei coefficienti:
print("\nüí° COME INTERPRETARE I COEFFICIENTI:")
print("‚Ä¢ Coefficiente POSITIVO: Aumenta la probabilit√† di churn (abbandono)")
print("‚Ä¢ Coefficiente NEGATIVO: Diminuisce la probabilit√† di churn")
print("‚Ä¢ Valore ASSOLUTO pi√π alto: Feature pi√π influente")

# Intercetta (bias)
print(f"\nüìê INTERCETTA (bias): {model.intercept_[0]:.4f}")
print("L'intercetta rappresenta la probabilit√† logistica di base quando tutte le feature sono 0")

# =============================================================================
# STEP 8: VALUTAZIONE DEL MODELLO
# =============================================================================

# Facciamo previsioni sul test set
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)  # Probabilit√† per ogni classe

print("\nüìà VALUTAZIONE DEL MODELLO:")

# 1. Accuratezza
accuracy = accuracy_score(y_test, y_pred)
print(f"üéØ ACCURATEZZA: {accuracy:.4f} ({accuracy*100:.2f}%)")

# 2. Matrice di confusione
print("\nüìä MATRICE DI CONFUSIONE:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# 3. Report di classificazione dettagliato
print("\nüìã REPORT DI CLASSIFICAZIONE:")
print(classification_report(y_test, y_pred))

# =============================================================================
# STEP 9: INTERPRETAZIONE DELLE PROBABILIT√Ä
# =============================================================================

# La regressione logistica fornisce probabilit√†, non solo classificazioni
print("\nüé≤ PROBABILIT√Ä DI PREDIZIONE (prime 5 istanze del test set):")
probabilities_df = pd.DataFrame({
    'Probabilit√† Classe 0 (No Churn)': y_pred_proba[:, 0],
    'Probabilit√† Classe 1 (Churn)': y_pred_proba[:, 1],
    'Predizione Finale': y_pred,
    'Valore Reale': y_test.values
})
print(probabilities_df.head())

# =============================================================================
# STEP 10: UTILIZZO CONCRETO DEL MODELLO
# =============================================================================

print("\nüöÄ UTILIZZO PRATICO DEL MODELLO:")

# Esempio: Prevedere se un NUOVO cliente abbandoner√†
nuovo_cliente = pd.DataFrame({
    'age': [45],
    'balance': [60000],
    'estimated_salary': [90000],
    'credit_score': [620],
    'tenure': [4],
    'number_of_products': [2],
    'has_credit_card': [1],
    'is_active_member': [1]
})

# Preprocessing del nuovo cliente
nuovo_cliente_scaled = scaler.transform(nuovo_cliente)

# Previsione
probabilita_churn = model.predict_proba(nuovo_cliente_scaled)[0, 1]
predizione = model.predict(nuovo_cliente_scaled)[0]

print(f"\nüîÆ PREVISIONE PER NUOVO CLIENTE:")
print(f"Probabilit√† di abbandono: {probabilita_churn:.4f} ({probabilita_churn*100:.2f}%)")
print(f"Predizione: {'ABBADONER√Ä' if predizione == 1 else 'RIMARR√Ä'}")

# Soglia di decisione (default 0.5, ma possiamo modificarla)
soglia_personalizzata = 0.3  # Pi√π sensibile ai casi di churn
predizione_soglia = (probabilita_churn > soglia_personalizzata).astype(int)

print(f"\nüéöÔ∏è CON SOGLIA PERSONALIZZATA ({soglia_personalizzata}):")
print(f"Predizione: {'ABBADONER√Ä' if predizione_soglia == 1 else 'RIMARR√Ä'}")

# =============================================================================
# STEP 11: QUANDO USARE E NON USARE LA REGRESSIONE LOGISTICA
# =============================================================================

print("\n" + "="*60)
print("üéØ QUANDO USARE LA REGRESSIONE LOGISTICA:")
print("="*60)
print("‚úÖ PROBLEMI DI CLASSIFICAZIONE BINARIA")
print("‚úÖ INTERPRETABILIT√Ä DEL MODELLO IMPORTANTE")
print("‚úÖ RELAZIONI APPROSSIMATIVAMENTE LINEARI")
print("‚úÖ DATASET DI DIMENSIONI PICCOLE/MEDIE")
print("‚úÖ BASELINE MODEL per confrontare modelli pi√π complessi")

print("\n" + "="*60)
print("üö´ QUANDO NON USARE LA REGRESSIONE LOGISTICA:")
print("="*60)
print("‚ùå PROBLEMI DI CLASSIFICAZIONE MULTI-CLASSE (pi√π di 2 categorie)")
print("‚ùå RELAZIONI COMPLESSE NON LINEARI")
print("‚ùå DATASET MOLTO GRANDI E COMPLESSI")
print("‚ùå FEATURE CON INTERAZIONI COMPLESSE")
print("‚ùå PERFORMANCE MASSIME RICHIESTE (usare XGBoost, Neural Networks)")
